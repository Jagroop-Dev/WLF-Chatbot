{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7856ab-f4be-47cb-93a9-d70d8b2080d1",
   "metadata": {},
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5b8913-441e-4ba8-a4fb-c3f2047bd7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\js208\\anaconda3\\lib\\site-packages (4.55.2)\n",
      "Requirement already satisfied: torch in c:\\users\\js208\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\js208\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\js208\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\js208\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\js208\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain langchain_community langchain_huggingface faiss-cpu\n",
    "#!pip install sentence-transformers\n",
    "#!pip install hf_xet\n",
    "#!pip install pypdf\n",
    "#!pip install SpeechRecognition pyttsx3\n",
    "#!pip install transformers torch faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9397ab-e29c-4070-8cb7-6c47bc4ed9bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1628d8bd-2250-4036-82ee-55904edb0abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01ef8d-1823-4994-a5d3-cd095ae401ac",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4bce899-3e11-496d-a550-fd100fd3eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"../Cleaned Data/\"\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents=loader.load()\n",
    "    return documents\n",
    "\n",
    "documents=load_pdf_files(data=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0355ec-ca4e-4aee-9086-853338fc9133",
   "metadata": {},
   "source": [
    "# Creating Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474491d8-726b-415e-82eb-434e733fafb4",
   "metadata": {},
   "source": [
    "Find that answers are missing context, increase the `chunk_overlap`\n",
    "Find that the chatbot is retrieving too much irrelevant information, decrease the `chunk_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e3e57-132b-4b69-90ce-bdda08a2adfd",
   "metadata": {},
   "source": [
    "Test one - `chunk_size` = 500, `chunk_overlap`=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fbd93-87b6-45dd-9c19-2c1bc38bb275",
   "metadata": {},
   "source": [
    "`chunk_overlap` is used to retain continuity and context(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c13f46f-0fb5-4615-91a9-0fe7d208cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69a48192-60f4-4923-9f35-093a9e8e526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text chunks 1915\n"
     ]
    }
   ],
   "source": [
    "text_chunks= create_chunks(documents)\n",
    "print(f\"Length of text chunks {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec848953-8eb7-4bae-b818-ebca9a8100d4",
   "metadata": {},
   "source": [
    "# Creating Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c06c019-3d93-4a15-a706-6467ec8c4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ve_model():\n",
    "    ve_model= HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "    return ve_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3160591e-468c-4d41-8232-94c6eae7c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = get_ve_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d2a2f-3f7d-4a78-aa1b-4bef53b8aa90",
   "metadata": {},
   "source": [
    "# Storing Vector Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f4f789a-4cf5-4976-99f2-186f4999848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAISS_DB_PATH = 'FAISS Database/'\n",
    "\n",
    "db=FAISS.from_documents(text_chunks, embedding_model)\n",
    "db.save_local(FAISS_DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f844e579-32e3-41a8-a1e6-974c6d2669c3",
   "metadata": {},
   "source": [
    "# Handling Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50f52c83-e850-4a9e-a1df-8148842e26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEmbeddings(Embeddings):\n",
    "    def __init__(self, model, processor):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "    \n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        pil_images = [Image.open(path) for path in texts]\n",
    "        inputs = self.processor(images=pil_images, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "        return image_features.cpu().numpy().tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        inputs = self.processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "        return text_features.cpu().numpy().tolist()[0]\n",
    "\n",
    "def load_image_files(base_image_path, model, processor):\n",
    "    all_image_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_image_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_image_paths.append(os.path.join(root, filename))\n",
    "\n",
    "    if not all_image_paths:\n",
    "        print(f\"No images found in {base_image_path}\")\n",
    "        return None\n",
    "\n",
    "    documents = [Document(page_content=path, metadata={\"source\": path}) for path in all_image_paths]\n",
    "\n",
    "    clip_embeddings = CLIPEmbeddings(model=model, processor=processor)\n",
    "\n",
    "    faiss_db = FAISS.from_documents(documents, clip_embeddings)\n",
    "    \n",
    "    print(\"FAISS image database created successfully.\")\n",
    "    return faiss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a946bff6-8e4d-4849-ade6-faa6233ea698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model():\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55a3a42e-6a38-40bb-8004-58af44966ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding_with_clip(text, model, processor):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "    return text_features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48b0e827-239b-4c0b-8238-0dc20de643c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding_with_clip(image_path, model, processor):\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    return image_features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af4e6a1f-bee4-45be-9d30-379f4c1dd43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 205 images. Processing embeddings...\n",
      "FAISS image database created successfully.\n"
     ]
    }
   ],
   "source": [
    "BASE_IMAGE_PATH = '../Data/Images'\n",
    "clip_model, clip_processor = load_clip_model()\n",
    "image_db = load_image_files(BASE_IMAGE_PATH, clip_model, clip_processor)\n",
    "\n",
    "IMAGE_FAISS_DB_PATH = 'FAISS_Image_DB/'\n",
    "if not os.path.exists(IMAGE_FAISS_DB_PATH):\n",
    "    os.makedirs(IMAGE_FAISS_DB_PATH)\n",
    "image_db.save_local(IMAGE_FAISS_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ca42b-33f6-4b63-a819-6f93825c708c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
